# AudioEngine Documentation for Kanka

## ðŸ“‹ Table of Contents
1. [Overview](#overview)
2. [Architecture](#architecture)
3. [WebSocket Relay Compatibility](#websocket-relay-compatibility)
4. [Core Components](#core-components)
5. [Configuration](#configuration)
6. [Usage Examples](#usage-examples)
7. [Testing Support](#testing-support)
8. [Performance Considerations](#performance-considerations)
9. [API Reference](#api-reference)

## ðŸŽ¯ Overview

AudioEngine is a **server-side audio processing framework** designed for Kanka's real-time voice chat system. It handles audio processing, validation, and transformation without dealing with platform-specific audio capture or playback.

### Key Principles
- **Processes audio bytes** received from any client (web, mobile, desktop)
- **Platform-agnostic** - works with audio data regardless of source
- **WebSocket-ready** - designed for streaming architectures
- **Testing-friendly** - includes optional desktop capture for development

### What AudioEngine Does
âœ… Validates audio format and quality  
âœ… Performs Voice Activity Detection (VAD)  
âœ… Manages audio buffers for streaming  
âœ… Handles audio encoding/decoding (when needed)  
âœ… Provides metrics and monitoring  

### What AudioEngine Doesn't Do
âŒ Capture audio from devices (that's client-side)  
âŒ Handle WebSocket connections directly  
âŒ Manage platform-specific APIs  
âŒ Store audio files  

## ðŸ—ï¸ Architecture

### AudioEngine in Kanka's Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Client      â”‚         â”‚          Kanka Backend              â”‚         â”‚   Provider  â”‚
â”‚                 â”‚         â”‚                                     â”‚         â”‚     API     â”‚
â”‚ [Web/Mobile]    â”‚  Audio  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   WS    â”‚             â”‚
â”‚ â€¢ Record Audio  â”‚ â”€â”€â”€â”€â”€â”€> â”‚  â”‚  WebSocket  â”‚  â”‚ AudioEngine â”‚ â”‚ â”€â”€â”€â”€â”€â”€> â”‚  OpenAI/    â”‚
â”‚ â€¢ Send Bytes   â”‚   WS     â”‚  â”‚    Relay    â”‚â†’ â”‚ (Process)   â”‚ â”‚         â”‚  Others     â”‚
â”‚ â€¢ Play TTS     â”‚ <â”€â”€â”€â”€â”€â”€ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ <â”€â”€â”€â”€â”€â”€ â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚                                     â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
                            â”‚  â”‚   Context System (Async)     â”‚   â”‚
                            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Hierarchy

```
AudioEngine/
â”œâ”€â”€ Core Processing
â”‚   â”œâ”€â”€ AudioProcessor      # Format validation, conversion
â”‚   â”œâ”€â”€ AudioStreamBuffer   # Streaming buffer management
â”‚   â””â”€â”€ BufferPool         # Zero-copy buffer allocation
â”‚
â”œâ”€â”€ Audio Analysis
â”‚   â”œâ”€â”€ FastVADDetector    # Voice Activity Detection
â”‚   â””â”€â”€ AudioMetrics       # Quality and performance metrics
â”‚
â”œâ”€â”€ Optional Components (Testing)
â”‚   â”œâ”€â”€ DirectAudioCapture # Desktop microphone capture
â”‚   â””â”€â”€ BufferedPlayer     # Local audio playback
â”‚
â””â”€â”€ Interfaces
    â”œâ”€â”€ AudioConfig        # Configuration
    â””â”€â”€ AudioTypes         # Type definitions
```

## ðŸ”Œ WebSocket Relay Compatibility

AudioEngine is **fully compatible** with WebSocket relay architecture. Here's how:

### Integration Pattern

```python
# WebSocket Relay using AudioEngine
class KankaWebSocketRelay:
    def __init__(self):
        # Initialize AudioEngine for processing
        self.audio_engine = AudioEngine(
            mode=ProcessingMode.REALTIME,
            config=AudioConfig(
                sample_rate=24000,
                chunk_duration_ms=20
            )
        )
        
        # Initialize with VAD
        self.audio_engine.configure_vad(
            VADConfig(
                type=VADType.ENERGY_BASED,
                speech_end_ms=500
            )
        )
    
    async def handle_client_audio(self, client_id: str, audio_bytes: bytes):
        """Process audio from client before forwarding to API"""
        
        # 1. Validate audio format
        if not self.audio_engine.validate_audio(audio_bytes):
            await self.send_error(client_id, "Invalid audio format")
            return
        
        # 2. Process through AudioEngine (minimal latency path)
        processed = self.audio_engine.process_audio(audio_bytes)
        
        # 3. Check VAD state
        vad_state = self.audio_engine.process_vad_chunk(processed)
        
        # 4. Forward to API immediately
        await self.api_connections[client_id].send(processed)
        
        # 5. Update context asynchronously (non-blocking)
        if vad_state == "speech":
            asyncio.create_task(
                self.context_manager.update(client_id, processed)
            )
    
    async def handle_api_audio(self, client_id: str, audio_bytes: bytes):
        """Process audio from API before sending to client"""
        
        # Minimal processing for TTS audio
        prepared = self.audio_engine.prepare_for_playback(audio_bytes)
        
        # Send to client
        await self.clients[client_id].send_bytes(prepared)
```

### Why It's Compatible

1. **Stateless Processing**: Each audio chunk is processed independently
2. **Async-Friendly**: All heavy operations can be offloaded
3. **Minimal Latency**: Fast-lane processing adds <5ms
4. **Binary Data**: Works directly with WebSocket binary frames
5. **Metrics**: Provides latency tracking for monitoring

## ðŸ”§ Core Components

### 1. AudioProcessor
Handles format validation, conversion, and quality processing.

```python
# Fast path processing
processor = AudioProcessor(mode=ProcessingMode.REALTIME)
valid, error = processor.validate_format(audio_bytes)
chunks = processor.chunk_audio(audio_bytes, chunk_duration_ms=20)
```

### 2. FastVADDetector
Lightweight Voice Activity Detection with state machine.

```python
vad = FastVADDetector(config=VADConfig(
    energy_threshold=0.02,
    speech_start_ms=100,
    speech_end_ms=500
))

state = vad.process_chunk(audio_chunk)  # Returns: SILENCE, SPEECH_STARTING, SPEECH, SPEECH_ENDING
```

### 3. AudioStreamBuffer
Manages streaming audio with configurable buffering strategies.

```python
buffer = AudioStreamBuffer(
    config=BufferConfig(
        max_size_bytes=1024*1024,  # 1MB
        overflow_strategy="drop_oldest"
    )
)
```

### 4. BufferedAudioPlayer (Testing Only)
For local playback during development.

```python
if TESTING_MODE:
    player = BufferedAudioPlayer(config=audio_config)
    player.play(audio_chunk)
```

## âš™ï¸ Configuration

### Basic Configuration

```python
from audioengine import AudioEngine, AudioConfig, ProcessingMode

# Production configuration
engine = AudioEngine(
    mode=ProcessingMode.REALTIME,  # Optimize for low latency
    config=AudioConfig(
        sample_rate=24000,          # OpenAI Realtime standard
        channels=1,                 # Mono for voice
        chunk_duration_ms=20,       # 20ms chunks for low latency
        format=AudioFormat.PCM16    # Standard format
    )
)
```

### VAD Configuration

```python
from audioengine import VADConfig, VADType

engine.configure_vad(
    VADConfig(
        type=VADType.ENERGY_BASED,
        energy_threshold=0.02,      # Sensitivity (0-1)
        speech_start_ms=100,        # Debounce for speech start
        speech_end_ms=500,          # Silence before ending speech
        pre_buffer_ms=300,          # Capture before speech detected
        adaptive=True               # Adjust to noise levels
    )
)
```

### Processing Modes

| Mode | Latency | Use Case |
|------|---------|----------|
| `REALTIME` | <5ms | Live conversation |
| `BALANCED` | ~20ms | Adaptive quality |
| `QUALITY` | ~50ms | Audio analysis |

## ðŸ“ Usage Examples

### Example 1: Basic Audio Processing

```python
# Initialize engine
engine = AudioEngine(mode=ProcessingMode.REALTIME)

# Process audio from client
audio_bytes = receive_from_client()  # Your WebSocket receive

# Validate and process
if engine.validate_audio(audio_bytes):
    processed = engine.process_audio(audio_bytes)
    
    # Check VAD
    vad_state = engine.process_vad_chunk(processed)
    
    if vad_state in ["speech", "speech_starting"]:
        # Forward to API
        await send_to_api(processed)
```

### Example 2: Streaming with Buffering

```python
# For handling jittery connections
engine = AudioEngine(mode=ProcessingMode.BALANCED)

# Add to stream buffer
audio_chunk = engine.add_to_stream_buffer(client_audio)

# Returns processed audio when buffer is ready
if audio_chunk:
    await forward_to_api(audio_chunk)
```

### Example 3: Metrics and Monitoring

```python
# Get comprehensive metrics
metrics = engine.get_metrics()
print(f"Average latency: {metrics['avg_latency_ms']}ms")
print(f"VAD state: {metrics.get('vad', {}).get('current_state')}")
print(f"Buffer status: {metrics.get('stream_buffer', {})}")

# Performance report
report = engine.get_performance_report()
if not report['performance']['realtime_capable']:
    logger.warning("Not meeting realtime requirements!")
```

### Example 4: TTS Audio Handling

```python
# Prepare TTS audio for client playback
tts_audio = receive_from_api()  # Opus or PCM16

# Queue for smooth playback (if testing locally)
engine.queue_playback(tts_audio)
engine.mark_playback_complete()

# Or just validate before sending to client
if engine.validate_audio(tts_audio):
    await send_to_client(tts_audio)
```

## ðŸ§ª Testing Support

### Local Testing with Desktop Capture

```python
# Enable local testing mode
import os
os.environ['ENABLE_LOCAL_TESTING'] = '1'

# Create test engine
engine = create_fast_lane_engine(
    input_device=None,  # Use default mic
    enable_local_testing=True
)

# Simulate client audio stream
async def test_with_microphone():
    audio_stream = await engine.start_capture_stream()
    
    async for chunk in audio_stream:
        # Process as if from client
        processed = engine.process_audio(chunk)
        print(f"Processed {len(processed)} bytes")
        
        # Simulate API interaction
        vad_state = engine.process_vad_chunk(processed)
        if vad_state == "speech":
            print("Speech detected!")
```

### Unit Testing

```python
# Test audio processing without hardware
def test_audio_processing():
    engine = AudioEngine(mode=ProcessingMode.REALTIME)
    
    # Create test audio (1 second of silence)
    test_audio = bytes(48000)  # 24kHz * 2 bytes * 1 second
    
    # Test validation
    assert engine.validate_audio(test_audio)
    
    # Test chunking
    chunks = list(engine.chunk_audio_generator(test_audio, chunk_ms=20))
    assert len(chunks) == 50  # 1000ms / 20ms
    
    # Test VAD
    vad_state = engine.process_vad_chunk(test_audio[:960])  # 20ms
    assert vad_state == "silence"
```

## ðŸ“Š Performance Considerations

### Latency Targets

| Operation | Target | Actual |
|-----------|--------|--------|
| Audio validation | <1ms | ~0.5ms |
| VAD processing | <2ms | ~1ms |
| Chunk processing | <5ms | ~3ms |
| Buffer operations | <1ms | ~0.2ms |

### Optimization Tips

1. **Use REALTIME mode** for production
2. **Disable numpy** for faster processing
3. **Use appropriate chunk sizes** (20-50ms)
4. **Enable buffer pool** for zero-copy operations
5. **Process VAD asynchronously** when possible

### Memory Usage

```python
# Check memory usage
metrics = engine.get_metrics()
buffer_pool_usage = metrics.get('buffer_pool', {})
print(f"Buffers in use: {buffer_pool_usage.get('in_use', 0)}")
print(f"Stream buffer: {metrics.get('stream_buffer', {}).get('available_bytes', 0)} bytes")
```

## ðŸ“š API Reference

### Core Classes

#### `AudioEngine`
Main orchestrator for audio processing.

```python
class AudioEngine:
    def __init__(self, mode: ProcessingMode = ProcessingMode.BALANCED,
                 config: AudioConfig = None,
                 buffer_config: BufferConfig = None,
                 logger: Optional[logging.Logger] = None)
```

**Key Methods:**

| Method | Description | Returns |
|--------|-------------|---------|
| `process_audio(audio_bytes: bytes)` | Process audio through appropriate pipeline | `bytes` |
| `validate_audio(audio_bytes: bytes)` | Validate audio format and quality | `bool` |
| `process_vad_chunk(audio_chunk: bytes)` | Process chunk through VAD | `str` (state) |
| `chunk_audio_generator(audio: bytes, chunk_ms: int)` | Generate audio chunks | `Generator[bytes]` |
| `configure_vad(vad_config: VADConfig)` | Configure Voice Activity Detection | `None` |
| `add_to_stream_buffer(audio: bytes)` | Add to streaming buffer | `Optional[bytes]` |
| `get_metrics()` | Get comprehensive metrics | `Dict[str, Any]` |
| `get_performance_report()` | Get performance analysis | `Dict[str, Any]` |
| `optimize_for_latency()` | Optimize for minimum latency | `None` |
| `optimize_for_quality()` | Optimize for best quality | `None` |

**Testing Methods (when enabled):**

| Method | Description | Returns |
|--------|-------------|---------|
| `start_capture_stream()` | Start local audio capture | `AsyncIterator[bytes]` |
| `stop_capture_stream()` | Stop audio capture | `None` |
| `queue_playback(audio: bytes)` | Queue audio for playback | `None` |
| `mark_playback_complete()` | Mark end of playback stream | `None` |
| `interrupt_playback(force: bool)` | Stop current playback | `None` |

#### `AudioProcessor`
Low-level audio processing operations.

```python
class AudioProcessor:
    def __init__(self, config: AudioConfig = None,
                 mode: ProcessingMode = ProcessingMode.BALANCED,
                 logger: Optional[logging.Logger] = None)
```

**Key Methods:**

| Method | Description | Returns |
|--------|-------------|---------|
| `validate_format(audio: bytes, expected: AudioFormat)` | Validate audio format | `Tuple[bool, Optional[str]]` |
| `process_realtime(audio: bytes)` | Fast-lane processing | `bytes` |
| `process_quality(audio: bytes, enhance: bool, normalize: bool)` | Quality processing | `Tuple[bytes, AudioMetadata]` |
| `chunk_audio(audio: bytes, chunk_duration_ms: int)` | Split into chunks | `List[bytes]` |
| `calculate_duration(audio: bytes)` | Calculate duration in ms | `float` |
| `resample(audio: bytes, from_rate: int, to_rate: int)` | Resample audio | `bytes` |
| `ensure_mono(audio: bytes, channels: int)` | Convert to mono | `bytes` |
| `analyze_audio(audio: bytes, detailed: bool)` | Analyze audio properties | `AudioMetadata` |

#### `FastVADDetector`
Voice Activity Detection with minimal latency.

```python
class FastVADDetector:
    def __init__(self, config: Optional[VADConfig] = None,
                 audio_config: Optional[AudioConfig] = None,
                 on_speech_start: Optional[Callable] = None,
                 on_speech_end: Optional[Callable] = None)
```

**Key Methods:**

| Method | Description | Returns |
|--------|-------------|---------|
| `process_chunk(audio_chunk: bytes)` | Process audio chunk | `VADState` |
| `get_pre_buffer()` | Get pre-speech buffer | `Optional[bytes]` |
| `reset()` | Reset VAD state | `None` |
| `get_metrics()` | Get VAD metrics | `Dict[str, Any]` |

#### `AudioStreamBuffer`
Buffer management for streaming scenarios.

```python
class AudioStreamBuffer:
    def __init__(self, config: BufferConfig = None,
                 audio_config: AudioConfig = None,
                 mode: ProcessingMode = ProcessingMode.BALANCED)
```

**Key Methods:**

| Method | Description | Returns |
|--------|-------------|---------|
| `add_audio(audio: bytes)` | Add audio to buffer | `bool` |
| `get_chunk(chunk_size: int)` | Get chunk from buffer | `Optional[bytes]` |
| `get_available_bytes()` | Get buffered byte count | `int` |
| `get_duration_ms()` | Get buffered duration | `float` |
| `clear()` | Clear buffer | `None` |
| `get_stats()` | Get buffer statistics | `Dict[str, Any]` |

### Configuration Classes

#### `AudioConfig`
```python
@dataclass
class AudioConfig:
    sample_rate: int = 24000      # Sample rate in Hz
    channels: int = 1             # Number of channels
    bit_depth: int = 16          # Bits per sample
    format: AudioFormat = AudioFormat.PCM16
    chunk_duration_ms: int = 100  # Default chunk size
    min_amplitude: float = 0.01   # Minimum for speech
    max_amplitude: float = 0.95   # Maximum before clipping
    use_numpy: bool = True        # Use numpy optimization
    pre_allocate_buffers: bool = True
```

#### `VADConfig`
```python
@dataclass
class VADConfig:
    type: VADType = VADType.ENERGY_BASED
    energy_threshold: float = 0.02     # 0.0-1.0
    speech_start_ms: int = 100         # Debounce start
    speech_end_ms: int = 500          # Debounce end  
    pre_buffer_ms: int = 300          # Pre-speech buffer
    adaptive: bool = False            # Adaptive threshold
```

#### `BufferConfig`
```python
@dataclass
class BufferConfig:
    max_size_bytes: int = 1024 * 1024  # 1MB
    max_duration_ms: int = 5000        # 5 seconds
    overflow_strategy: str = "drop_oldest"  # drop_oldest/drop_newest/error
    underflow_strategy: str = "silence"     # silence/repeat/error
    pre_allocate: bool = True
    use_circular: bool = True
    zero_copy: bool = True
```

### Enums

#### `ProcessingMode`
```python
class ProcessingMode(Enum):
    REALTIME = "realtime"    # <20ms latency
    QUALITY = "quality"      # ~200ms, best quality
    BALANCED = "balanced"    # ~50ms, adaptive
```

#### `VADState`
```python
class VADState(Enum):
    SILENCE = "silence"
    SPEECH_STARTING = "speech_starting"
    SPEECH = "speech"
    SPEECH_ENDING = "speech_ending"
```

#### `AudioFormat`
```python
class AudioFormat(Enum):
    PCM16 = "pcm16"
    G711_ULAW = "g711_ulaw"
    G711_ALAW = "g711_alaw"
```

### Factory Functions

#### `create_fast_lane_engine()`
```python
def create_fast_lane_engine(
    sample_rate: int = 24000,
    chunk_duration_ms: int = 20,
    vad_config: Optional[VADConfig] = None
) -> AudioEngine
```
Creates an engine optimized for minimal latency.

#### `create_big_lane_engine()`
```python
def create_big_lane_engine(
    sample_rate: int = 24000,
    enable_enhancement: bool = True
) -> AudioEngine
```
Creates an engine optimized for quality processing.

#### `create_adaptive_engine()`
```python
def create_adaptive_engine(
    sample_rate: int = 24000,
    latency_target_ms: float = 50.0
) -> AudioEngine
```
Creates an engine that adapts based on performance.

## ðŸŽ¯ Summary

AudioEngine provides a robust, server-side audio processing framework that:
- âœ… **Integrates seamlessly** with WebSocket relay architecture
- âœ… **Processes audio** with <5ms latency in fast-lane mode
- âœ… **Validates and chunks** audio for streaming
- âœ… **Detects speech** with configurable VAD
- âœ… **Provides metrics** for monitoring
- âœ… **Supports testing** with optional desktop capture

It's designed to be the audio processing layer in Kanka's backend, handling all audio-related operations while remaining agnostic to transport and platform concerns.